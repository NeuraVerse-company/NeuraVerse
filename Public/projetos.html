<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projetos - Neuraverse</title>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background: #000000;
            color: #e0e0e0;
            position: relative;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }
        #background-canvas {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
        }
        #mic-button {
            padding: 15px;
            background-color: #0078d4;
            color: white;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            font-size: 24px;
            width: 60px;
            height: 60px;
            display: flex;
            justify-content: center;
            align-items: center;
            margin-bottom: 10px;
            z-index: 2;
        }
        #mic-button.listening {
            background-color: #ff4444;
        }
        #status {
            color: #ff4444;
            font-size: 14px;
            text-align: center;
            display: block;
            margin-bottom: 20px;
            z-index: 2;
        }
    </style>
</head>
<body>
    <canvas id="background-canvas"></canvas>
    <button id="mic-button">🎤</button>
    <div id="status">Clique no botão para ativar o microfone e começar a conversar.</div>

    <script>
        let isSpeechSDKLoaded = false;

        function onSpeechSDKLoaded() {
            console.log("Azure Speech SDK carregado com sucesso!");
            isSpeechSDKLoaded = true;
        }
    </script>
    <script src="https://unpkg.com/microsoft-cognitiveservices-speech-sdk@1.43.0/distrib/browser/microsoft.cognitiveservices.speech.sdk.bundle-min.js" onload="onSpeechSDKLoaded()"></script>
    <script>
        const backgroundCanvas = document.getElementById('background-canvas');
        const backgroundCtx = backgroundCanvas.getContext('2d');
        let backgroundParticlesArray = [];

        function resizeBackgroundCanvas() {
            backgroundCanvas.width = window.innerWidth;
            backgroundCanvas.height = window.innerHeight;
            backgroundCtx.fillStyle = '#000000';
            backgroundCtx.fillRect(0, 0, backgroundCanvas.width, backgroundCanvas.height);
        }
        window.addEventListener('resize', resizeBackgroundCanvas);
        resizeBackgroundCanvas();

        class Particle {
            constructor(canvas) {
                this.canvas = canvas;
                this.x = Math.random() * canvas.width;
                this.y = Math.random() * canvas.height;
                this.size = Math.random() * 5 + 1;
                this.speedX = Math.random() * 2 - 1;
                this.speedY = Math.random() * 2 - 1;
                this.brightness = Math.random() * 0.5 + 0.5;
            }
            update() {
                this.x += this.speedX;
                this.y += this.speedY;
                if (this.x > this.canvas.width || this.x < 0) this.speedX *= -1;
                if (this.y > this.canvas.height || this.y < 0) this.speedY *= -1;
                this.brightness = Math.sin(Date.now() * 0.005) * 0.5 + 0.5;
            }
            draw(ctx) {
                ctx.fillStyle = `rgba(0, 183, 255, ${this.brightness})`;
                ctx.beginPath();
                ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2);
                ctx.fill();
            }
        }

        function initBackgroundParticles() {
            backgroundParticlesArray = [];
            const numberOfParticles = 50;
            for (let i = 0; i < numberOfParticles; i++) {
                backgroundParticlesArray.push(new Particle(backgroundCanvas));
            }
        }
        initBackgroundParticles();

        function connectParticles(particles, ctx, canvas) {
            for (let a = 0; a < particles.length; a++) {
                for (let b = a; b < particles.length; b++) {
                    let dx = particles[a].x - particles[b].x;
                    let dy = particles[a].y - particles[b].y;
                    let distance = Math.sqrt(dx * dx + dy * dy);
                    if (distance < 100) {
                        ctx.strokeStyle = `rgba(0, 183, 255, ${1 - distance / 100})`;
                        ctx.lineWidth = 1;
                        ctx.beginPath();
                        ctx.moveTo(particles[a].x, particles[a].y);
                        ctx.lineTo(particles[b].x, particles[b].y);
                        ctx.stroke();
                    }
                }
            }
        }

        function animate() {
            backgroundCtx.clearRect(0, 0, backgroundCanvas.width, backgroundCanvas.height);
            backgroundCtx.fillStyle = '#000000';
            backgroundCtx.fillRect(0, 0, backgroundCanvas.width, backgroundCanvas.height);
            for (let i = 0; i < backgroundParticlesArray.length; i++) {
                backgroundParticlesArray[i].update();
                backgroundParticlesArray[i].draw(backgroundCtx);
            }
            connectParticles(backgroundParticlesArray, backgroundCtx, backgroundCanvas);
            requestAnimationFrame(animate);
        }
        animate();

        // Lógica para reconhecimento de fala no frontend
        const micButton = document.getElementById("mic-button");
        const status = document.getElementById("status");
        let ws = null;
        let isRecognizing = false;
        let recognizer = null;
        let keys = null;

        async function getKeys() {
            try {
                const response = await fetch("/api/keys");
                const data = await response.json();
                keys = data;
                return data;
            } catch (err) {
                console.error("Erro ao buscar as chaves:", err);
                status.textContent = "Erro ao buscar as chaves: " + err.message;
                return null;
            }
        }

        function connectWebSocket() {
            ws = new WebSocket("wss://neuraverse-company-cvdncgayb0a0fgek.brazilsouth-01.azurewebsites.net");

            ws.onopen = () => {
                console.log("Conectado ao servidor WebSocket");
                status.textContent = "Conectado ao WebSocket. Clique no botão para falar.";
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                if (data.text && data.response) {
                    status.textContent = "Você disse: " + data.text + "\nResposta: " + data.response;
                    speakResponse(data.response);
                } else if (data.error) {
                    status.textContent = data.error;
                    isRecognizing = false;
                    micButton.classList.remove("listening");
                    if (recognizer) {
                        recognizer.stopContinuousRecognitionAsync();
                        recognizer.close();
                        recognizer = null;
                    }
                }
            };

            ws.onclose = () => {
                console.log("Desconectado do servidor WebSocket");
                isRecognizing = false;
                micButton.classList.remove("listening");
                status.textContent = "Clique no botão para ativar o microfone e começar a conversar.";
                if (recognizer) {
                    recognizer.stopContinuousRecognitionAsync();
                    recognizer.close();
                    recognizer = null;
                }
            };

            ws.onerror = (error) => {
                console.error("Erro no WebSocket:", error);
                status.textContent = "Erro ao conectar ao WebSocket: " + error.message;
            };
        }

        async function startRecognition() {
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                connectWebSocket();
            }

            setTimeout(async () => {
                if (ws.readyState !== WebSocket.OPEN) {
                    status.textContent = "Erro: WebSocket não está conectado.";
                    return;
                }

                if (!keys) {
                    await getKeys();
                }
                if (!keys || !keys.speechKey || !keys.speechRegion) {
                    status.textContent = "Erro: Chave ou região do Speech Service não disponível.";
                    return;
                }

                if (!isSpeechSDKLoaded) {
                    status.textContent = "Erro: Azure Speech SDK ainda não foi carregado. Tente novamente.";
                    return;
                }

                const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(keys.speechKey, keys.speechRegion);
                speechConfig.speechRecognitionLanguage = "pt-BR";
                const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
                recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);

                recognizer.recognized = (s, e) => {
                    if (e.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
                        console.log("Texto reconhecido:", e.result.text);
                        ws.send(JSON.stringify({ text: e.result.text }));
                    }
                };

                recognizer.canceled = (s, e) => {
                    console.error("Reconhecimento cancelado:", e.errorDetails);
                    ws.send(JSON.stringify({ error: "Erro no reconhecimento de voz: " + e.errorDetails }));
                    recognizer.close();
                    recognizer = null;
                    isRecognizing = false;
                    micButton.classList.remove("listening");
                };

                recognizer.sessionStopped = () => {
                    console.log("Sessão de reconhecimento parada.");
                    recognizer.close();
                    recognizer = null;
                    isRecognizing = false;
                    micButton.classList.remove("listening");
                };

                recognizer.startContinuousRecognitionAsync(
                    () => {
                        console.log("Reconhecimento iniciado com sucesso!");
                        status.textContent = "Microfone ativado. Fale algo...";
                    },
                    (err) => {
                        console.error("Erro ao iniciar reconhecimento:", err);
                        status.textContent = "Erro ao iniciar o reconhecimento: " + err;
                    }
                );

                isRecognizing = true;
                micButton.classList.add("listening");
            }, 500);
        }

        function stopRecognition() {
            if (recognizer) {
                recognizer.stopContinuousRecognitionAsync(
                    () => {
                        console.log("Reconhecimento parado.");
                        recognizer.close();
                        recognizer = null;
                    },
                    (err) => console.error("Erro ao parar reconhecimento:", err)
                );
            }
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ action: "stop" }));
            }
            isRecognizing = false;
            micButton.classList.remove("listening");
            status.textContent = "Clique no botão para ativar o microfone e começar a conversar.";
        }

        async function speakResponse(text) {
            if (!keys || !keys.elevenlabsKey || !keys.elevenlabsVoiceId) {
                status.textContent = "Erro: Chaves do ElevenLabs não disponíveis.";
                return;
            }

            const url = `https://api.elevenlabs.io/v1/text-to-speech/${keys.elevenlabsVoiceId}/stream`;

            try {
                console.log("Enviando solicitação para ElevenLabs:", text);
                const response = await fetch(url, {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                        "xi-api-key": keys.elevenlabsKey
                    },
                    body: JSON.stringify({
                        text: text,
                        model_id: "eleven_multilingual_v2",
                        voice_settings: { stability: 0.5, similarity_boost: 0.5 }
                    })
                });

                if (!response.ok) throw new Error("Erro na resposta do ElevenLabs: " + response.statusText);
                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                audio.play();
                audio.onplay = () => console.log("Áudio sendo reproduzido.");
                audio.onerror = (err) => {
                    console.error("Erro ao reproduzir áudio:", err);
                    status.textContent = "Erro ao reproduzir a resposta de áudio.";
                };
            } catch (err) {
                console.error("Erro ao sintetizar voz com ElevenLabs:", err);
                status.textContent = "Erro ao sintetizar a voz: " + err.message;
            }
        }

        micButton.addEventListener("click", () => {
            if (!isRecognizing) {
                startRecognition();
            } else {
                stopRecognition();
            }
        });
    </script>
</body>
</html>